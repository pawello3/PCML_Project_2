{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n",
      "Shape of ratings matrix: (10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "dataset_file_path = \"data_train.csv\"\n",
    "ratings = load_data(dataset_file_path)\n",
    "print('Shape of ratings matrix:', ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEbCAYAAABgLnslAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxhhjYssaKWOMMbH1/7lQg4nq7jY6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f07495d7748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min # of items per user = 8, min # of users per item = 3.\n"
     ]
    }
   ],
   "source": [
    "from plots import plot_raw_data\n",
    "\n",
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_matrices_random(train, num_features):\n",
    "    \"\"\"\n",
    "        Initialize randomly matrices W and Z of matrix factorization.\n",
    "\n",
    "        Arguments:\n",
    "            train: training set (matrix X)\n",
    "            num_features: number of latent variables in the W*Z^T decomposition\n",
    "\n",
    "        Returned value(s):\n",
    "            item_features: matrix W of shape = num_features, num_item\n",
    "            user_features: matrix Z of shape = num_features, num_user\n",
    "    \"\"\"\n",
    "    \n",
    "    # W matrix initialization\n",
    "    item_features = np.random.random((train.shape[0], num_features))\n",
    "    # Z matrix initialization\n",
    "    user_features = np.random.random((train.shape[1], num_features))\n",
    "    \n",
    "    return item_features, user_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_matrices_first_column_mean(train, num_features):\n",
    "    \"\"\"\n",
    "        Initialize randomly matrices W and Z of matrix factorization.\n",
    "        In matrix W first column is assigned to average rating for that movie.\n",
    "\n",
    "        Arguments:\n",
    "            train: training set (matrix X)\n",
    "            num_features: number of latent variables in the W*Z^T decomposition\n",
    "\n",
    "        Returned value(s):\n",
    "            item_features: matrix W of shape = num_features, num_item\n",
    "            user_features: matrix Z of shape = num_features, num_user\n",
    "    \"\"\"\n",
    "\n",
    "    # W matrix initialization\n",
    "    item_features = np.random.random((train.shape[0], num_features))\n",
    "    item_features[:, 0] = train.mean(axis=1).reshape(item_features.shape[0])\n",
    "    # Z matrix initialization\n",
    "    user_features = np.random.random((train.shape[1], num_features))\n",
    "\n",
    "    return item_features, user_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_matrices_global_mean(train, num_features):\n",
    "    \"\"\"\n",
    "        Initialize matrices W and Z of matrix factorization such that W*Z^T contains global mean\n",
    "        at all positions. Therefore all elements of W and Z equal to square root of global_mean/num_features\n",
    "\n",
    "        Arguments:\n",
    "            train: training set (matrix X)\n",
    "            num_features: number of latent variables in the W*Z^T decomposition\n",
    "\n",
    "        Returned value(s):\n",
    "            item_features: matrix W of shape = num_features, num_item\n",
    "            user_features: matrix Z of shape = num_features, num_user\n",
    "    \"\"\"\n",
    "    \n",
    "    global_mean = np.sum(train) / len(train.nonzero()[0])\n",
    "\n",
    "    # W matrix initialization\n",
    "    item_features = np.sqrt(global_mean/num_features) * np.ones((train.shape[0], num_features)) \n",
    "    # Z matrix initialization\n",
    "    user_features = np.sqrt(global_mean/num_features) * np.ones((train.shape[1], num_features))\n",
    "    \n",
    "    return item_features, user_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_matrices_SVD(train, num_features):\n",
    "    \"\"\"\n",
    "        Initialize matrices W and Z of matrix factorization using SVD decomposition of original matrix X.\n",
    "\n",
    "        Arguments:\n",
    "            train: training set (matrix X)\n",
    "            num_features: number of latent variables in the W*Z^T decomposition\n",
    "\n",
    "        Returned value(s):\n",
    "            item_features: matrix W of shape = num_features, num_item\n",
    "            user_features: matrix Z of shape = num_features, num_user\n",
    "    \"\"\"\n",
    "    \n",
    "    U, s, V = np.linalg.svd(train, full_matrices=False)\n",
    "    \n",
    "    S = np.diag(s)\n",
    "\n",
    "    U_1 = U[:, 0:num_features]\n",
    "    S_1 = S[0:num_features, 0:num_features]\n",
    "    V_1 = V[0:num_features, :]\n",
    "    \n",
    "    # W matrix initialization\n",
    "    item_features = U_1\n",
    "    # Z matrix initialization\n",
    "    user_features = (S_1.dot(V_1)).T\n",
    "    \n",
    "    return item_features, user_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_ALS(train_set, train_nonzero_indices, test_set, test_nonzero_indices, num_epochs, cutoff, max_iter_threshold, num_features, item_features, user_features, test_mode):\n",
    "    # initialize matrices used to compute RMSE\n",
    "    train_label = np.zeros(len(train_nonzero_indices))\n",
    "    test_label = np.zeros(len(test_nonzero_indices))\n",
    "    train_prediction_label = np.zeros(len(train_nonzero_indices))\n",
    "    test_prediction_label = np.zeros(len(test_nonzero_indices))\n",
    "    \n",
    "    # initialize accumulators for RMSE of every iteration\n",
    "    train_rmse = np.zeros(num_epochs)\n",
    "    test_rmse = np.zeros(num_epochs)\n",
    "    if test_mode == True:\n",
    "        for i in range(len(test_nonzero_indices)):\n",
    "            test_label[i] = test_set[test_nonzero_indices[i][0], test_nonzero_indices[i][1]]\n",
    "    \n",
    "    lambda_user_diag = np.identity(num_features)\n",
    "    np.fill_diagonal(lambda_user_diag, lambda_user)\n",
    "    lambda_item_diag = np.identity(num_features)\n",
    "    np.fill_diagonal(lambda_item_diag, lambda_user)\n",
    "    \n",
    "    last_train_rmse = 0\n",
    "    \n",
    "    for it in range(num_epochs):\n",
    "        begin = datetime.datetime.now() # start time measurement\n",
    "        \n",
    "        print(\"Epoch:\", it)\n",
    "\n",
    "        # perform one iteteration of the algorithm\n",
    "        \n",
    "        # first fix item features: Z^T = (W^T*W + (lambda_z*I_K)^(-1)*W^T*X)\n",
    "        user_features = (np.linalg.inv(item_features.T.dot(item_features) + lambda_user_diag).dot(item_features.T.dot(train_set))).T\n",
    "        # then fix user features: W^T = (Z^T*Z + (lambda_w*I_K)^(-1)*Z^T*X^T)\n",
    "        item_features = (np.linalg.inv(user_features.T.dot(user_features) + lambda_item_diag).dot(user_features.T.dot(train_set.T))).T\n",
    "\n",
    "        # calculate training RMSE\n",
    "        for i in range(len(train_nonzero_indices)):\n",
    "            train_label[i] = train_set[train_nonzero_indices[i][0], train_nonzero_indices[i][1]]\n",
    "            train_prediction_label[i] = item_features[train_nonzero_indices[i][0], :].dot(user_features.T[:, train_nonzero_indices[i][1]])\n",
    "        \n",
    "        # store train RMSE of current iteration\n",
    "        train_rmse[it] = calculate_mse(train_label, train_prediction_label)\n",
    "        \n",
    "        print(\"RMSE on training set:\", train_rmse[it])\n",
    "        \n",
    "        if test_mode == True:\n",
    "            # calculate test RMSE\n",
    "            for i in range(len(test_nonzero_indices)):\n",
    "                test_prediction_label[i] = item_features[test_nonzero_indices[i][0], :].dot(user_features.T[:, test_nonzero_indices[i][1]])\n",
    "\n",
    "            # store test RMSE of current iteration\n",
    "            test_rmse[it] = calculate_mse(test_label, test_prediction_label)\n",
    "\n",
    "            print(\"RMSE on test set:\", test_rmse[it])\n",
    "\n",
    "        end = datetime.datetime.now() # stop time measurement\n",
    "        \n",
    "        # compute the time of the iteration\n",
    "        execution_time = (end - begin).total_seconds()\n",
    "        print(\"Execution time:\", execution_time)\n",
    "\n",
    "        print(\"*\" * 50)\n",
    "        \n",
    "        if np.fabs(last_train_rmse - train_rmse[it]) < max_iter_threshold:\n",
    "            print(\"ALREADY\")\n",
    "            if cutoff == True:\n",
    "                break\n",
    "        else:\n",
    "            last_train_rmse = train_rmse[it]\n",
    "    \n",
    "    return item_features.dot(user_features.T), train_rmse, test_rmse, it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "RMSE on training set: 0.987939023755\n",
      "Execution time: 6.631406\n",
      "**************************************************\n",
      "Epoch: 1\n",
      "RMSE on training set: 0.901150818631\n",
      "Execution time: 6.450634\n",
      "**************************************************\n",
      "Epoch: 2\n",
      "RMSE on training set: 0.888434744609\n",
      "Execution time: 6.56809\n",
      "**************************************************\n",
      "Epoch: 3\n",
      "RMSE on training set: 0.88483451742\n",
      "Execution time: 6.392751\n",
      "**************************************************\n",
      "Epoch: 4\n",
      "RMSE on training set: 0.883383315355\n",
      "Execution time: 6.56982\n",
      "**************************************************\n",
      "Epoch: 5\n",
      "RMSE on training set: 0.882720365411\n",
      "Execution time: 6.52346\n",
      "**************************************************\n",
      "Epoch: 6\n",
      "RMSE on training set: 0.882387965847\n",
      "Execution time: 6.623516\n",
      "**************************************************\n",
      "Epoch: 7\n",
      "RMSE on training set: 0.882209636636\n",
      "Execution time: 6.372072\n",
      "**************************************************\n",
      "Epoch: 8\n",
      "RMSE on training set: 0.882109664487\n",
      "Execution time: 6.624325\n",
      "**************************************************\n",
      "Epoch: 9\n",
      "RMSE on training set: 0.882052466814\n",
      "Execution time: 6.580088\n",
      "**************************************************\n",
      "Epoch: 10\n",
      "RMSE on training set: 0.882019894297\n",
      "Execution time: 6.611417\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 11\n",
      "RMSE on training set: 0.882001978114\n",
      "Execution time: 7.078764\n",
      "**************************************************\n",
      "Epoch: 12\n",
      "RMSE on training set: 0.881992889073\n",
      "Execution time: 6.768953\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 13\n",
      "RMSE on training set: 0.881989055066\n",
      "Execution time: 6.928757\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 14\n",
      "RMSE on training set: 0.881988220675\n",
      "Execution time: 7.219661\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 15\n",
      "RMSE on training set: 0.881988941689\n",
      "Execution time: 6.461882\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 16\n",
      "RMSE on training set: 0.881990293849\n",
      "Execution time: 6.642886\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 17\n",
      "RMSE on training set: 0.881991694656\n",
      "Execution time: 6.659965\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 18\n",
      "RMSE on training set: 0.881992789057\n",
      "Execution time: 6.665631\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 19\n",
      "RMSE on training set: 0.881993373663\n",
      "Execution time: 6.478241\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 20\n",
      "RMSE on training set: 0.881993345602\n",
      "Execution time: 7.051205\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 21\n",
      "RMSE on training set: 0.881992667839\n",
      "Execution time: 6.62994\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 22\n",
      "RMSE on training set: 0.881991345696\n",
      "Execution time: 6.755779\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 23\n",
      "RMSE on training set: 0.881989411077\n",
      "Execution time: 7.008664\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 24\n",
      "RMSE on training set: 0.881986911907\n",
      "Execution time: 6.687016\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 25\n",
      "RMSE on training set: 0.881983905085\n",
      "Execution time: 6.419813\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 26\n",
      "RMSE on training set: 0.881980451779\n",
      "Execution time: 6.821385\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 27\n",
      "RMSE on training set: 0.881976614267\n",
      "Execution time: 6.540987\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 28\n",
      "RMSE on training set: 0.881972453829\n",
      "Execution time: 6.461023\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 29\n",
      "RMSE on training set: 0.88196802933\n",
      "Execution time: 14.297473\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 30\n",
      "RMSE on training set: 0.881963396305\n",
      "Execution time: 6.697269\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 31\n",
      "RMSE on training set: 0.881958606381\n",
      "Execution time: 6.369392\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 32\n",
      "RMSE on training set: 0.88195370696\n",
      "Execution time: 14.754424\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 33\n",
      "RMSE on training set: 0.881948741085\n",
      "Execution time: 6.554791\n",
      "**************************************************\n",
      "Epoch: 34\n",
      "RMSE on training set: 0.88194374744\n",
      "Execution time: 6.624573\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 35\n",
      "RMSE on training set: 0.88193876046\n",
      "Execution time: 6.516078\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 36\n",
      "RMSE on training set: 0.881933810496\n",
      "Execution time: 6.706484\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 37\n",
      "RMSE on training set: 0.881928924044\n",
      "Execution time: 6.714419\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 38\n",
      "RMSE on training set: 0.88192412399\n",
      "Execution time: 7.211343\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 39\n",
      "RMSE on training set: 0.881919429878\n",
      "Execution time: 6.646442\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 40\n",
      "RMSE on training set: 0.881914858178\n",
      "Execution time: 7.003692\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 41\n",
      "RMSE on training set: 0.881910422555\n",
      "Execution time: 6.499707\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 42\n",
      "RMSE on training set: 0.881906134119\n",
      "Execution time: 6.524825\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 43\n",
      "RMSE on training set: 0.881902001672\n",
      "Execution time: 6.389502\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 44\n",
      "RMSE on training set: 0.881898031938\n",
      "Execution time: 6.469626\n",
      "**************************************************\n",
      "Epoch: 45\n",
      "RMSE on training set: 0.88189422977\n",
      "Execution time: 6.354986\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 46\n",
      "RMSE on training set: 0.881890598354\n",
      "Execution time: 6.575521\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 47\n",
      "RMSE on training set: 0.881887139384\n",
      "Execution time: 7.026763\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 48\n",
      "RMSE on training set: 0.881883853237\n",
      "Execution time: 6.549318\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 49\n",
      "RMSE on training set: 0.881880739122\n",
      "Execution time: 6.334714\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 50\n",
      "RMSE on training set: 0.881877795226\n",
      "Execution time: 6.586054\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 51\n",
      "RMSE on training set: 0.881875018848\n",
      "Execution time: 6.357193\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 52\n",
      "RMSE on training set: 0.881872406521\n",
      "Execution time: 6.404002\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 53\n",
      "RMSE on training set: 0.881869954124\n",
      "Execution time: 6.513334\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 54\n",
      "RMSE on training set: 0.881867656991\n",
      "Execution time: 6.554149\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 55\n",
      "RMSE on training set: 0.881865510007\n",
      "Execution time: 6.515588\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 56\n",
      "RMSE on training set: 0.881863507699\n",
      "Execution time: 6.750219\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 57\n",
      "RMSE on training set: 0.881861644316\n",
      "Execution time: 6.304898\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 58\n",
      "RMSE on training set: 0.881859913911\n",
      "Execution time: 6.510281\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 59\n",
      "RMSE on training set: 0.881858310402\n",
      "Execution time: 6.644805\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 60\n",
      "RMSE on training set: 0.88185682764\n",
      "Execution time: 6.817117\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 61\n",
      "RMSE on training set: 0.881855459461\n",
      "Execution time: 6.436897\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 62\n",
      "RMSE on training set: 0.881854199735\n",
      "Execution time: 6.555065\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 63\n",
      "RMSE on training set: 0.881853042412\n",
      "Execution time: 6.942421\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 64\n",
      "RMSE on training set: 0.881851981553\n",
      "Execution time: 6.570892\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 65\n",
      "RMSE on training set: 0.881851011364\n",
      "Execution time: 6.427339\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 66\n",
      "RMSE on training set: 0.881850126221\n",
      "Execution time: 6.730763\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 67\n",
      "RMSE on training set: 0.881849320693\n",
      "Execution time: 6.437208\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 68\n",
      "RMSE on training set: 0.881848589554\n",
      "Execution time: 6.541558\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 69\n",
      "RMSE on training set: 0.881847927798\n",
      "Execution time: 11.948701\n",
      "**************************************************\n",
      "Epoch: 70\n",
      "RMSE on training set: 0.881847330648\n",
      "Execution time: 6.610266\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 71\n",
      "RMSE on training set: 0.881846793559\n",
      "Execution time: 6.392104\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 72\n",
      "RMSE on training set: 0.88184631222\n",
      "Execution time: 6.689162\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 73\n",
      "RMSE on training set: 0.881845882559\n",
      "Execution time: 6.404582\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 74\n",
      "RMSE on training set: 0.881845500733\n",
      "Execution time: 14.775567\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 75\n",
      "RMSE on training set: 0.88184516313\n",
      "Execution time: 6.525109\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 76\n",
      "RMSE on training set: 0.881844866362\n",
      "Execution time: 6.563176\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 77\n",
      "RMSE on training set: 0.881844607256\n",
      "Execution time: 6.614093\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 78\n",
      "RMSE on training set: 0.88184438285\n",
      "Execution time: 6.705937\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 79\n",
      "RMSE on training set: 0.881844190381\n",
      "Execution time: 10.330432\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 80\n",
      "RMSE on training set: 0.88184402728\n",
      "Execution time: 6.688557\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 81\n",
      "RMSE on training set: 0.881843891159\n",
      "Execution time: 6.39407\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 82\n",
      "RMSE on training set: 0.881843779804\n",
      "Execution time: 6.602392\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 83\n",
      "RMSE on training set: 0.881843691166\n",
      "Execution time: 6.547823\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 84\n",
      "RMSE on training set: 0.881843623348\n",
      "Execution time: 6.559764\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 85\n",
      "RMSE on training set: 0.881843574601\n",
      "Execution time: 6.385126\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 86\n",
      "RMSE on training set: 0.88184354331\n",
      "Execution time: 6.673714\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 87\n",
      "RMSE on training set: 0.881843527991\n",
      "Execution time: 12.184858\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 88\n",
      "RMSE on training set: 0.881843527274\n",
      "Execution time: 6.892707\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 89\n",
      "RMSE on training set: 0.881843539905\n",
      "Execution time: 6.700035\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 90\n",
      "RMSE on training set: 0.881843564729\n",
      "Execution time: 6.684511\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 91\n",
      "RMSE on training set: 0.881843600689\n",
      "Execution time: 6.372294\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 92\n",
      "RMSE on training set: 0.881843646815\n",
      "Execution time: 6.662812\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 93\n",
      "RMSE on training set: 0.881843702221\n",
      "Execution time: 6.432007\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 94\n",
      "RMSE on training set: 0.881843766095\n",
      "Execution time: 6.636321\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 95\n",
      "RMSE on training set: 0.881843837695\n",
      "Execution time: 6.43628\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 96\n",
      "RMSE on training set: 0.881843916345\n",
      "Execution time: 6.708608\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 97\n",
      "RMSE on training set: 0.881844001427\n",
      "Execution time: 6.239099\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 98\n",
      "RMSE on training set: 0.881844092377\n",
      "Execution time: 6.628793\n",
      "**************************************************\n",
      "ALREADY\n",
      "Epoch: 99\n",
      "RMSE on training set: 0.881844188682\n",
      "Execution time: 6.867603\n",
      "**************************************************\n",
      "ALREADY\n"
     ]
    }
   ],
   "source": [
    "# set random seed\n",
    "np.random.seed(888)\n",
    "\n",
    "# define parameters\n",
    "init_method_num = 1 # number of matrices initialization method\n",
    "num_epochs = 100 # number of iterations of ALS\n",
    "cutoff = False # setting for usage of max_iter_threshold stop condition\n",
    "max_iter_threshold = 0.00005 # stop condition for ALS algorithm, no visible improvement\n",
    "num_features = 50 # number of latent features in matrix factorization\n",
    "lambda_item = 60 # regularization parameter for item features\n",
    "lambda_user = 10 # regularization parameter for user features\n",
    "split_ratio = 1.0 # ratio between size of training and test set\n",
    "test_mode = True\n",
    "if split_ratio == 1.0:\n",
    "    test_mode = False\n",
    "\n",
    "def nonzero_indices(matrix):\n",
    "    nz_row, nz_col = matrix.nonzero()\n",
    "    return list(zip(nz_row, nz_col))\n",
    "\n",
    "# find the non-zero ratings indices in the training set\n",
    "nonzero_indices = nonzero_indices(ratings)\n",
    "\n",
    "# convert sparse matrix representation to dense matrix representation\n",
    "ratings_dense = scipy.sparse.lil_matrix.todense(ratings)\n",
    "\n",
    "# preprocessing\n",
    "initialize_methods = [initialize_matrices_random, initialize_matrices_first_column_mean, initialize_matrices_global_mean, initialize_matrices_SVD]\n",
    "item_features, user_features = None, None\n",
    "\n",
    "if init_method_num != 2:\n",
    "    # initialize matrices W and Z\n",
    "    item_features, user_features = initialize_methods[init_method_num](ratings_dense, num_features)\n",
    "\n",
    "# normalize rows of ratings matrix by substracting mean (bias) rating for each movie\n",
    "h = np.nanmean(np.where(ratings_dense != 0, ratings_dense, np.nan), axis = 0)\n",
    "for i, j in nonzero_indices:\n",
    "    ratings_dense[i, j] -= h[j]\n",
    "\n",
    "# normalize columns of ratings matrix by substracting mean (bias) rating for each users\n",
    "v = np.nanmean(np.where(ratings_dense != 0, ratings_dense, np.nan), axis = 1)\n",
    "for i, j in nonzero_indices:\n",
    "    ratings_dense[i, j] -= v[i]\n",
    "\n",
    "if init_method_num == 2:\n",
    "    # initialize matrices W and Z\n",
    "    item_features, user_features = initialize_methods[init_method_num](ratings_dense, num_features)\n",
    "\n",
    "# split data into training and test sets\n",
    "np.random.shuffle(nonzero_indices)\n",
    "\n",
    "split_point = int(np.floor(len(nonzero_indices) * split_ratio))\n",
    "train_nonzero_indices = nonzero_indices[:split_point]\n",
    "test_nonzero_indices = nonzero_indices[split_point:]\n",
    "\n",
    "train_set = np.zeros(ratings_dense.shape)\n",
    "test_set = np.zeros(ratings_dense.shape)\n",
    "\n",
    "for i, j in train_nonzero_indices:\n",
    "    train_set[i, j] = ratings_dense[i, j]\n",
    "\n",
    "for i, j in test_nonzero_indices:\n",
    "    test_set[i, j] = ratings_dense[i, j]\n",
    "\n",
    "# compute the prediction and errors\n",
    "prediction, train_rmse, test_rmse, num_iter = compute_ALS(train_set, train_nonzero_indices, test_set, test_nonzero_indices, num_epochs, cutoff, max_iter_threshold, num_features, item_features, user_features, test_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0723054a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if test_mode == True:\n",
    "    plt.plot(range(num_iter), train_rmse[:num_iter], range(num_iter), test_rmse[:num_iter])\n",
    "else:\n",
    "    plt.plot(range(num_iter), train_rmse[:num_iter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_csv_submission(prediction, submission_file_path = \"submission.csv\"):\n",
    "    \"\"\"\n",
    "        Creates an output file in csv format for submission to kaggle.\n",
    "\n",
    "        Arguments:\n",
    "            prediction: matrix W * Z^T\n",
    "            submission_file_path: string name of .csv output file to be created\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_file_path = \"sampleSubmission.csv\" # file path to the dataset of the entries to be predicted\n",
    "    sample_ratings = load_data(dataset_file_path)\n",
    "    \n",
    "    # find the non-zero ratings indices \n",
    "    nz_row_sr, nz_col_sr = sample_ratings.nonzero()\n",
    "    nz_sr = list(zip(nz_row_sr, nz_col_sr))\n",
    "    \n",
    "    def trim_values(x):\n",
    "        if x < 1:\n",
    "            return 1\n",
    "        if x > 5:\n",
    "            return 5\n",
    "        return x\n",
    "    \n",
    "    def discretize_values(x):\n",
    "        if x <= 1.5:\n",
    "            return 1\n",
    "        if x <= 2.5:\n",
    "            return 2\n",
    "        if x <= 3.5:\n",
    "            return 3\n",
    "        if x <= 4.5:\n",
    "            return 4\n",
    "        return 5\n",
    "    \n",
    "    submission_file_path = time.strftime(\"%Y%m%d_%H%M%S\") + \" \" + submission_file_path\n",
    "    with open(submission_file_path, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for i, j in nz_sr:\n",
    "            writer.writerow({'Id' : 'r' + str(i + 1) + '_' + 'c' + str(j + 1),\n",
    "                             'Prediction' : str(trim_values(prediction[i, j]))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "# denormalize rows of ratings matrix by adding back mean (bias) rating for each movie\n",
    "H = np.zeros(prediction.shape)\n",
    "for i in range(prediction.shape[0]):\n",
    "    H[i, :] = h\n",
    "\n",
    "# denormalize columns of ratings matrix by adding back mean (bias) rating for each user\n",
    "V = np.zeros(prediction.shape)\n",
    "for i in range(prediction.shape[1]):\n",
    "    V[:, i] = v\n",
    "\n",
    "prediction = prediction + H + V\n",
    "\n",
    "if test_mode == False:\n",
    "    create_csv_submission(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
